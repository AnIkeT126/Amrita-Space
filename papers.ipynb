{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prQULF_bYqKg"
      },
      "source": [
        "# Paper\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ0Mw8HHYqKl"
      },
      "source": [
        "# Contents\n",
        "\n",
        "In this Notebook we will cover the Following:\n",
        "* Simple RNN's\n",
        "* Word Embeddings : Definition and How to get them\n",
        "* LSTM's\n",
        "* GRU's\n",
        "* BI-Directional RNN's\n",
        "* Encoder-Decoder Models (Seq2Seq Models)\n",
        "* Attention Models\n",
        "* Transformers - Attention is all you need\n",
        "* BERT\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "rglQVicPYqKn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, GRU,SimpleRNN\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.utils import to_categorical\n",
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
        "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
        "from keras.preprocessing import sequence, text\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from plotly import graph_objs as go\n",
        "import plotly.express as px\n",
        "import plotly.figure_factory as ff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZUTr3DyYqKp"
      },
      "source": [
        "# Configuring TPU's\n",
        "\n",
        "For this version of Notebook we will be using TPU's as we have to built a BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9E54LY0YqKp",
        "outputId": "49b813ae-67c3-4798-94ec-2be60064c37d",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "REPLICAS:  1\n"
          ]
        }
      ],
      "source": [
        "# Detect hardware, return appropriate distribution strategy\n",
        "try:\n",
        "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
        "    # set: this is always the case on Kaggle.\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "UIPBCXJtYqKq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('jigsaw-toxic-comment-train.csv')\n",
        "validation = pd.read_csv('test.csv')\n",
        "test = pd.read_csv('validation.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywwRH8ISYqKq"
      },
      "source": [
        "We will drop the other columns and approach this problem as a Binary Classification Problem and also we will have our exercise done on a smaller subsection of the dataset(only 12000 data points) to make it easier to train the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NcEfFQuCYqKr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train.drop(['severe_toxic','obscene','threat','insult','identity_hate'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpVGaM69YqKr",
        "outputId": "1e1462c0-ddb9-46ad-de7c-1dba446c9ad6",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(12001, 3)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = train.loc[:12000,:]\n",
        "train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGj4h-iMYqKs"
      },
      "source": [
        "We will check the maximum number of words that can be present in a comment , this will help us in padding later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8YU7rpgYqKs",
        "outputId": "bc0cd490-6b97-4bda-e912-af87783bcc5c",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1403"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train['comment_text'].apply(lambda x:len(str(x).split())).max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRBCVAVOYqKs"
      },
      "source": [
        "Writing a function for getting auc score for validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UUfQFPbzYqKs",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def roc_auc(predictions,target):\n",
        "    '''\n",
        "    This methods returns the AUC Score when given the Predictions\n",
        "    and Labels\n",
        "    '''\n",
        "\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(target, predictions)\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "    return roc_auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykl4Ra0VYqKs"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "blSQ5FvFYqKt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "xtrain, xvalid, ytrain, yvalid = train_test_split(train.comment_text.values, train.toxic.values,\n",
        "                                                  stratify=train.toxic.values,\n",
        "                                                  random_state=42,\n",
        "                                                  test_size=0.2, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FWZn2aGAnZL2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ewa5l_Bynl9t",
        "outputId": "b84b70fc-e366-4b96-e84a-0606e5dd4789"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'pip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!pip install Keras-Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-xCucSd8n9BF"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import pad_sequences\n",
        "from keras_preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rJ6lk-jEYqKt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# using keras tokenizer here\n",
        "token = text.Tokenizer(num_words=None)\n",
        "max_len = 1500\n",
        "\n",
        "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
        "xtrain_seq = token.texts_to_sequences(xtrain)\n",
        "xvalid_seq = token.texts_to_sequences(xvalid)\n",
        "\n",
        "#zero pad the sequences\n",
        "xtrain_pad = tf.keras.preprocessing.sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
        "xvalid_pad = tf.keras.preprocessing.sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
        "word_index = token.word_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezW8W9vIyL9S"
      },
      "source": [
        "# **RNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbzewTt9YqKt",
        "outputId": "4654f34b-a299-4f70-b697-b33ef3ba5533",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 1500, 300)         13049100  \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 100)               40100     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,089,301\n",
            "Trainable params: 13,089,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "CPU times: total: 156 ms\n",
            "Wall time: 265 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "with strategy.scope():\n",
        "    # A simpleRNN without any pretrained embeddings and one dense layer\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(word_index) + 1,\n",
        "                     300,\n",
        "                     input_length=max_len))\n",
        "    model.add(SimpleRNN(100))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjINdYhCYqKu",
        "outputId": "2a65a973-7a91-418d-fb79-1c092437e0a6",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "150/150 [==============================] - 278s 2s/step - loss: 0.3436 - accuracy: 0.8756\n",
            "Epoch 2/3\n",
            "150/150 [==============================] - 381s 3s/step - loss: 0.4580 - accuracy: 0.8435\n",
            "Epoch 3/3\n",
            "150/150 [==============================] - 394s 3s/step - loss: 0.2791 - accuracy: 0.9424\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x158aff2b910>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(xtrain_pad, ytrain, epochs=3, batch_size=64 * strategy.num_replicas_in_sync)\n",
        " #Multiplying by Strategy to run on TPU's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rFxlvYmYqKu",
        "outputId": "f778ec05-3ee4-4a28-862f-be7f2a4919ce",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "76/76 [==============================] - 38s 495ms/step\n",
            "Auc: 0.71%\n"
          ]
        }
      ],
      "source": [
        "scores = model.predict(xvalid_pad)\n",
        "print(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "DOHZkLuFYqKu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "scores_model = []\n",
        "scores_model.append({'Model': 'SimpleRNN','AUC_Score': roc_auc(scores,yvalid)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDjxQX0rYqKu"
      },
      "source": [
        "## Code Explanantion\n",
        "* Tokenization<br><br>\n",
        " So if you have watched the videos and referred to the links, you would know that in an RNN we input a sentence word by word. We represent every word as one hot vectors of dimensions : Numbers of words in Vocab +1. <br>\n",
        "  What keras Tokenizer does is , it takes all the unique words in the corpus,forms a dictionary with words as keys and their number of occurences as values,it then sorts the dictionary in descending order of counts. It then assigns the first value 1 , second value 2 and so on. So let's suppose word 'the' occured the most in the corpus then it will assigned index 1 and vector representing 'the' would be a one-hot vector with value 1 at position 1 and rest zereos.<br>\n",
        "  Try printing first 2 elements of xtrain_seq you will see every word is represented as a digit now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9FzWriwYqKu",
        "outputId": "39d039aa-c482-4291-f330-30bf8691e74f",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[664,\n",
              "  65,\n",
              "  7,\n",
              "  19,\n",
              "  2262,\n",
              "  14102,\n",
              "  5,\n",
              "  2262,\n",
              "  20439,\n",
              "  6071,\n",
              "  4,\n",
              "  71,\n",
              "  32,\n",
              "  20440,\n",
              "  6620,\n",
              "  39,\n",
              "  6,\n",
              "  664,\n",
              "  65,\n",
              "  11,\n",
              "  8,\n",
              "  20441,\n",
              "  1502,\n",
              "  38,\n",
              "  6072]]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xtrain_seq[:1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qg4CfISEYqKu"
      },
      "source": [
        "<b>Now you might be wondering What is padding? Why its done</b><br><br>\n",
        "\n",
        "Here is the answer :\n",
        "* https://www.quora.com/Which-effect-does-sequence-padding-have-on-the-training-of-a-neural-network\n",
        "* https://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction/\n",
        "* https://www.coursera.org/lecture/natural-language-processing-tensorflow/padding-2Cyzs\n",
        "\n",
        "Also sometimes people might use special tokens while tokenizing like EOS(end of string) and BOS(Begining of string). Here is the reason why it's done\n",
        "* https://stackoverflow.com/questions/44579161/why-do-we-do-padding-in-nlp-tasks\n",
        "\n",
        "\n",
        "The code token.word_index simply gives the dictionary of vocab that keras created for us"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9qY62bhYqKu"
      },
      "source": [
        "* Building the Neural Network\n",
        "\n",
        "To understand the Dimensions of input and output given to RNN in keras her is a beautiful article : https://medium.com/@shivajbd/understanding-input-and-output-shape-in-lstm-keras-c501ee95c65e\n",
        "\n",
        "The first line model.Sequential() tells keras that we will be building our network sequentially . Then we first add the Embedding layer.\n",
        "Embedding layer is also a layer of neurons which takes in as input the nth dimensional one hot vector of every word and converts it into 300 dimensional vector , it gives us word embeddings similar to word2vec. We could have used word2vec but the embeddings layer learns during training to enhance the embeddings.\n",
        "Next we add an 100 LSTM units without any dropout or regularization\n",
        "At last we add a single neuron with sigmoid function which takes output from 100 LSTM cells (Please note we have 100 LSTM cells not layers) to predict the results and then we compile the model using adam optimizer\n",
        "\n",
        "* Comments on the model<br><br>\n",
        "We can see our model achieves an accuracy of 1 which is just insane , we are clearly overfitting I know , but this was the simplest model of all ,we can tune a lot of hyperparameters like RNN units, we can do batch normalization , dropouts etc to get better result. The point is we got an AUC score of 0.82 without much efforts and we know have learnt about RNN's .Deep learning is really revolutionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2cRcDAqYqKv"
      },
      "source": [
        "# Word Embeddings\n",
        "\n",
        "While building our simple RNN models we talked about using word-embeddings , So what is word-embeddings and how do we get word-embeddings?\n",
        "Here is the answer :\n",
        "* https://www.coursera.org/learn/nlp-sequence-models/lecture/6Oq70/word-representation\n",
        "* https://machinelearningmastery.com/what-are-word-embeddings/\n",
        "<br> <br>\n",
        "The latest approach to getting word Embeddings is using pretained GLoVe or using Fasttext. Without going into too much details, I would explain how to create sentence vectors and how can we use them to create a machine learning model on top of it and since I am a fan of GloVe vectors, word2vec and fasttext. In this Notebook, I'll be using the GloVe vectors. You can download the GloVe vectors from here http://www-nlp.stanford.edu/data/glove.840B.300d.zip or you can search for GloVe in datasets on Kaggle and add the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the GloVe vectors in a dictionary:\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open('/kaggle/input/glove840b300dtxt/glove.840B.300d.txt','r',encoding='utf-8')\n",
        "for line in tqdm(f):\n",
        "    values = line.split(' ')\n",
        "    word = values[0]\n",
        "    coefs = np.asarray([float(val) for val in values[1:]])\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7UVzuYOYqKv"
      },
      "source": [
        "# LSTM's\n",
        "\n",
        "## Basic Overview\n",
        "\n",
        "Simple RNN's were certainly better than classical ML algorithms and gave state of the art results, but it failed to capture long term dependencies that is present in sentences . So in 1998-99 LSTM's were introduced to counter to these drawbacks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xz7Y2578YqKv",
        "outputId": "7a91ea77-f676-4967-da4b-8e44fb33e077",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 43496/43496 [00:00<00:00, 2560210.04it/s]\n"
          ]
        }
      ],
      "source": [
        "# create an embedding matrix for the words we have in the dataset\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "for word, i in tqdm(word_index.items()):\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8eIf9EfYqKv",
        "outputId": "cca161b8-8d23-4a05-bef4-cd40d6e18164",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 1500, 300)         13049100  \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 100)               160400    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,209,601\n",
            "Trainable params: 160,501\n",
            "Non-trainable params: 13,049,100\n",
            "_________________________________________________________________\n",
            "CPU times: total: 266 ms\n",
            "Wall time: 200 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "with strategy.scope():\n",
        "\n",
        "    # A simple LSTM with glove embeddings and one dense layer\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(word_index) + 1,\n",
        "                     300,\n",
        "                     weights=[embedding_matrix],\n",
        "                     input_length=max_len,\n",
        "                     trainable=False))\n",
        "\n",
        "    model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "lsoIF8PsYqKv",
        "outputId": "9854125f-e1c6-432b-a42c-252e4867069b",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "147/150 [============================>.] - ETA: 2:49 - loss: 0.3392 - accuracy: 0.9048"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mfit(xtrain_pad, ytrain, epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m\u001b[39m*\u001b[39;49mstrategy\u001b[39m.\u001b[39;49mnum_replicas_in_sync)\n",
            "File \u001b[1;32mc:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
            "File \u001b[1;32mc:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32mc:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
            "File \u001b[1;32mc:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
            "File \u001b[1;32mc:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
            "File \u001b[1;32mc:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
            "File \u001b[1;32mc:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
            "File \u001b[1;32mc:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model.fit(xtrain_pad, ytrain, epochs=2, batch_size=64*strategy.num_replicas_in_sync)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "ReDnq2blYqKw",
        "outputId": "f3865b95-916c-4c3a-da41-fe9d553ac425",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "76/76 [==============================] - 37s 491ms/step\n",
            "Auc: 0.71%\n"
          ]
        }
      ],
      "source": [
        "scores = model.predict(xvalid_pad)\n",
        "print(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "TrJ9RzOwYqKw",
        "outputId": "7b550f54-f428-4d42-c754-3b4fb6112bd1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "scores_model.append({'Model': 'LSTM','AUC_Score': roc_auc(scores,yvalid)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZllBuf74YqKw"
      },
      "source": [
        "## Code Explanation\n",
        "\n",
        "As a first step we calculate embedding matrix for our vocabulary from the pretrained GLoVe vectors . Then while building the embedding layer we pass Embedding Matrix as weights to the layer instead of training it over Vocabulary and thus we pass trainable = False.\n",
        "Rest of the model is same as before except we have replaced the SimpleRNN By LSTM Units\n",
        "\n",
        "* Comments on the Model\n",
        "\n",
        "We now see that the model is not overfitting and achieves an auc score of 0.96 which is quite commendable , also we close in on the gap between accuracy and auc .\n",
        "We see that in this case we used dropout and prevented overfitting the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaCzkSu4YqKw"
      },
      "source": [
        "# GRU's\n",
        "\n",
        "## Basic  Overview\n",
        "\n",
        "Introduced by Cho, et al. in 2014, GRU (Gated Recurrent Unit) aims to solve the vanishing gradient problem which comes with a standard recurrent neural network. GRU's are a variation on the LSTM because both are designed similarly and, in some cases, produce equally excellent results . GRU's were designed to be simpler and faster than LSTM's and in most cases produce equally good results and thus there is no clear winner.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yA-8NJCbYqKw",
        "outputId": "5c69d9da-05ab-408a-e842-79ead0349a3b",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 1500, 300)         13049100  \n",
            "                                                                 \n",
            " spatial_dropout1d (SpatialD  (None, 1500, 300)        0         \n",
            " ropout1D)                                                       \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 300)               541800    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 301       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,591,201\n",
            "Trainable params: 542,101\n",
            "Non-trainable params: 13,049,100\n",
            "_________________________________________________________________\n",
            "CPU times: total: 562 ms\n",
            "Wall time: 301 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "with strategy.scope():\n",
        "    # GRU with glove embeddings and two dense layers\n",
        "     model = Sequential()\n",
        "     model.add(Embedding(len(word_index) + 1,\n",
        "                     300,\n",
        "                     weights=[embedding_matrix],\n",
        "                     input_length=max_len,\n",
        "                     trainable=False))\n",
        "     model.add(SpatialDropout1D(0.3))\n",
        "     model.add(GRU(300))\n",
        "     model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "     model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "VnWAs8pfYqKw",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "123/150 [=======================>......] - ETA: 32:12 - loss: 0.3311 - accuracy: 0.9045"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32md:\\7th Sem\\major project\\Dataset\\papers.ipynb Cell 40\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/7th%20Sem/major%20project/Dataset/papers.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(xtrain_pad, ytrain, epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m\u001b[39m*\u001b[39;49mstrategy\u001b[39m.\u001b[39;49mnum_replicas_in_sync)\n",
            "File \u001b[1;32mc:\\Users\\asus_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\asus_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
            "File \u001b[1;32mc:\\Users\\asus_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\asus_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32mc:\\Users\\asus_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
            "File \u001b[1;32mc:\\Users\\asus_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
            "File \u001b[1;32mc:\\Users\\asus_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
            "File \u001b[1;32mc:\\Users\\asus_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
            "File \u001b[1;32mc:\\Users\\asus_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "model.fit(xtrain_pad, ytrain, epochs=3, batch_size=64*strategy.num_replicas_in_sync)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ovGCMj-nYqKx",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "76/76 [==============================] - 248s 3s/step\n",
            "Auc: 0.50%\n"
          ]
        }
      ],
      "source": [
        "scores = model.predict(xvalid_pad)\n",
        "print(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "KKXCRjGzYqKy",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "scores_model.append({'Model': 'GRU','AUC_Score': roc_auc(scores,yvalid)})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "vaYT3LN7YqKy",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'Model': 'SimpleRNN', 'AUC_Score': 0.7062541286894779},\n",
              " {'Model': 'LSTM', 'AUC_Score': 0.7062541286894779},\n",
              " {'Model': 'GRU', 'AUC_Score': 0.5}]"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8GIT58eYqKy"
      },
      "source": [
        "# Bi-Directional RNN's\n",
        "\n",
        "\n",
        "## Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "8Ur8ScRdYqKy",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 1500, 300)         13049100  \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 600)              1442400   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 601       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,492,101\n",
            "Trainable params: 1,443,001\n",
            "Non-trainable params: 13,049,100\n",
            "_________________________________________________________________\n",
            "CPU times: total: 641 ms\n",
            "Wall time: 394 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "with strategy.scope():\n",
        "    # A simple bidirectional LSTM with glove embeddings and one dense layer\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(word_index) + 1,\n",
        "                     300,\n",
        "                     weights=[embedding_matrix],\n",
        "                     input_length=max_len,\n",
        "                     trainable=False))\n",
        "    model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
        "\n",
        "    model.add(Dense(1,activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.fit(xtrain_pad, ytrain, epochs=1, batch_size=64*strategy.num_replicas_in_sync)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scores = model.predict(xvalid_pad)\n",
        "print(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zu7XSAFiYqKz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "scores_model.append({'Model': 'Bi-directional LSTM','AUC_Score': roc_auc(scores,yvalid)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi20WwHYYqKz"
      },
      "source": [
        "## Code Explanation\n",
        "\n",
        "Code is same as before,only we have added bidirectional nature to the LSTM cells we used before and is self explanatory. We have achieve similar accuracy and auc score as before and now we have learned all the types of typical RNN architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUnQ3BnWYqKz"
      },
      "source": [
        "**We are now at the end of part 1 of this notebook and things are about to go wild now as we Enter more complex and State of the art models .If you have followed along from the starting and read all the articles and understood everything , these complex models would be fairly easy to understand.I recommend Finishing Part 1 before continuing as the upcoming techniques can be quite overwhelming**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
<<<<<<< HEAD
=======
        "id": "hD_PBThfYqKz"
      },
      "source": [

      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hs1pA4IXYqKz",
        "trusted": true
      },
      "outputs": [],
      "source": [
      
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GFCBcE-YqKz",
        "trusted": true
      },
      "outputs": [],
      "source": [
      
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
>>>>>>> c62f9a1c601d78e21ca8ccc7d93dbe4ca81dc5d7
        "id": "oLkSc_7-YqK0"
      },
      "source": [
        "# Attention Models\n",
        "\n",
        "This is the toughest and most tricky part. If you are able to understand the intiuition and working of attention block , understanding transformers and transformer based architectures like BERT will be a piece of cake. This is the part where I spent the most time on and I suggest you do the same . Please read and view the following resources in the order I am providing to ignore getting confused, also at the end of this try to write and draw an attention block in your own way :-\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLYgzT5bYqK0"
      },
      "source": [
        "# Transformers : Attention is all you need\n",
        "\n",
        "So finally we have reached the end of the learning curve and are about to start learning the technology that changed NLP completely and are the reasons for the state of the art NLP techniques ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBNXQkXsYqK0"
      },
      "source": [
        "# BERT and Its Implementation on this Competition\n",
        "\n",
        "\n",
        "Steps Involved :\n",
        "* Data Preparation : Tokenization and encoding of data\n",
        "* Configuring TPU's\n",
        "* Building a Function for Model Training and adding an output layer for classification\n",
        "* Train the model and get the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading Dependencies\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "\n",
        "from tokenizers import BertWordPieceTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pyV_Jqu6YqK1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# LOADING THE DATA\n",
        "\n",
        "train1 = pd.read_csv(\"jigsaw-toxic-comment-train.csv\")\n",
        "valid = pd.read_csv('validation.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "sub = pd.read_csv('sample_submission.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfQkQ745YqK1"
      },
      "source": [
        "Encoder FOr DATA for understanding waht encode batch does read documentation of hugging face tokenizer :\n",
        "https://huggingface.co/transformers/main_classes/tokenizer.html here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Pe-pO298YqK1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n",
        "    \"\"\"\n",
        "    Encoder for encoding the text into sequence of integers for BERT Input\n",
        "    \"\"\"\n",
        "    tokenizer.enable_truncation(max_length=maxlen)\n",
        "    tokenizer.enable_padding(max_length=maxlen)\n",
        "    all_ids = []\n",
        "\n",
        "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
        "        text_chunk = texts[i:i+chunk_size].tolist()\n",
        "        encs = tokenizer.encode_batch(text_chunk)\n",
        "        all_ids.extend([enc.ids for enc in encs])\n",
        "\n",
        "    return np.array(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "rZiDEaSQYqK1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#IMP DATA FOR CONFIG\n",
        "\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "\n",
        "# Configuration\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
        "MAX_LEN = 192"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMnYDMuIYqK1"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "For understanding please refer to hugging face documentation again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Tokenizer(vocabulary_size=119547, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=False, wordpieces_prefix=##)"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# First load the real tokenizer\n",
        "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
        "# Save the loaded tokenizer locally\n",
        "tokenizer.save_pretrained('.')\n",
        "# Reload it with the huggingface tokenizers library\n",
        "fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\n",
        "fast_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "# Create a DistilBERT tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
        "\n",
        "# Define your desired maximum length\n",
        "MAX_LEN = 128\n",
        "\n",
        "# Helper function to encode and pad sequences\n",
        "def encode_and_pad(texts):\n",
        "    # Encode the texts\n",
        "    encodings = tokenizer(texts.tolist(), padding=True, truncation=True, max_length=MAX_LEN, return_tensors='np')\n",
        "    \n",
        "    # Convert the attention mask to a NumPy array\n",
        "    attention_mask = np.array(encodings['attention_mask'])\n",
        "    \n",
        "    # Calculate the number of tokens to pad\n",
        "    num_tokens_to_pad = MAX_LEN - attention_mask.shape[1]\n",
        "    \n",
        "    # Pad the attention mask with zeros\n",
        "    attention_mask = np.pad(attention_mask, ((0, 0), (0, num_tokens_to_pad)), mode='constant')\n",
        "    \n",
        "    return {\n",
        "        'input_ids': np.array(encodings['input_ids']),\n",
        "        'attention_mask': attention_mask\n",
        "    }\n",
        "\n",
        "# Encode and pad the training, validation, and test data\n",
        "x_train = encode_and_pad(train1.comment_text.astype(str))\n",
        "x_valid = encode_and_pad(valid.comment_text.astype(str))\n",
        "x_test = encode_and_pad(test.content.astype(str))\n",
        "\n",
        "# Extract labels\n",
        "y_train = train1.toxic.values\n",
        "y_valid = valid.toxic.values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "3Z7aUyoyYqK2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((x_train, y_train))\n",
        "    .repeat()\n",
        "    .shuffle(2048)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "valid_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((x_valid, y_valid))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .cache()\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "test_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices(x_test)\n",
        "    .batch(BATCH_SIZE)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "ED377aD8YqK2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def build_model(transformer, max_len=512):\n",
        "    \"\"\"\n",
        "    function for training the BERT model\n",
        "    \"\"\"\n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    sequence_output = transformer(input_word_ids)[0]\n",
        "    cls_token = sequence_output[:, 0, :]\n",
        "    out = Dense(1, activation='sigmoid')(cls_token)\n",
        "\n",
        "    model = Model(inputs=input_word_ids, outputs=out)\n",
        "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxAd0EzjYqK2"
      },
      "source": [
        "## Starting Training\n",
        "\n",
        "If you want to use any another model just replace the model name in transformers._____ and use accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "FC6Lj-uiYqK2",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "765cb979c7f64cfb8c01210d7641f17d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/542M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_word_ids (InputLayer)  [(None, 128)]            0         \n",
            "                                                                 \n",
            " tf_distil_bert_model (TFDis  TFBaseModelOutput(last_h  134734080\n",
            " tilBertModel)               idden_state=(None, 128,             \n",
            "                             768),                               \n",
            "                              hidden_states=None, att            \n",
            "                             entions=None)                       \n",
            "                                                                 \n",
            " tf.__operators__.getitem (S  (None, 768)              0         \n",
            " licingOpLambda)                                                 \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 769       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 134,734,849\n",
            "Trainable params: 134,734,849\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "CPU times: total: 9.58 s\n",
            "Wall time: 7min 57s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\asus_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "with strategy.scope():\n",
        "    transformer_layer = (\n",
        "        transformers.TFDistilBertModel\n",
        "        .from_pretrained('distilbert-base-multilingual-cased')\n",
        "    )\n",
        "    model = build_model(transformer_layer, max_len=MAX_LEN)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_data = {\n",
        "    'input_word_ids': x_train['input_ids'],  # Use the appropriate key\n",
        "    'attention_mask': x_train['attention_mask']  # Use the appropriate key\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Check the data type of x_train\n",
        "if isinstance(x_train, dict):\n",
        "    # If x_train is a dictionary, extract the appropriate components\n",
        "    input_ids = x_train['input_ids']  # Replace 'input_ids' with the actual key\n",
        "    attention_mask = x_train['attention_mask']  # Replace 'attention_mask' with the actual key\n",
        "\n",
        "    # Convert the components to NumPy arrays\n",
        "    input_ids = np.array(input_ids)\n",
        "    attention_mask = np.array(attention_mask)\n",
        "\n",
        "    # Determine the number of steps based on the shape of input_ids\n",
        "    n_steps = input_ids.shape[0]\n",
        "else:\n",
        "    # If x_train is already a NumPy array or TensorFlow tensor, use it directly\n",
        "    n_steps = x_train.shape[0]\n",
        "\n",
        "# Now you can use n_steps in your model.fit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train for 1746 steps, validate for 63 steps\n",
            "Epoch 1/3\n",
            "1746/1746 [==============================] - 255s 146ms/step - loss: 0.1221 - accuracy: 0.9517 - val_loss: 0.4484 - val_accuracy: 0.8479\n",
            "Epoch 2/3\n",
            "1746/1746 [==============================] - 198s 114ms/step - loss: 0.0908 - accuracy: 0.9634 - val_loss: 0.4769 - val_accuracy: 0.8491\n",
            "Epoch 3/3\n",
            "1746/1746 [==============================] - 198s 113ms/step - loss: 0.0775 - accuracy: 0.9680 - val_loss: 0.5522 - val_accuracy: 0.8500\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_history = model.fit(train_dataset,steps_per_epoch=n_steps,validation_data=valid_dataset,epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train for 62 steps\n",
            "Epoch 1/6\n",
            "62/62 [==============================] - 18s 291ms/step - loss: 0.3244 - accuracy: 0.8613\n",
            "Epoch 2/6\n",
            "62/62 [==============================] - 25s 401ms/step - loss: 0.2354 - accuracy: 0.8955\n",
            "Epoch 3/6\n",
            "62/62 [==============================] - 7s 110ms/step - loss: 0.1718 - accuracy: 0.9252\n",
            "Epoch 4/6\n",
            "62/62 [==============================] - 7s 111ms/step - loss: 0.1210 - accuracy: 0.9492\n",
            "Epoch 5/6\n",
            "62/62 [==============================] - 7s 114ms/step - loss: 0.0798 - accuracy: 0.9686\n",
            "Epoch 6/6\n",
            "62/62 [==============================] - 7s 110ms/step - loss: 0.0765 - accuracy: 0.9696\n"
          ]
        }
      ],
      "source": [
        "n_steps = x_valid.shape[0] // BATCH_SIZE\n",
        "train_history_2 = model.fit(\n",
        "    valid_dataset.repeat(),\n",
        "    steps_per_epoch=n_steps,\n",
        "    epochs=EPOCHS*2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erSSxqFoYqK3"
      },
      "source": [
        "# End Notes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'Model': 'BERT', 'AUC_Score': 0.9282}]\n"
          ]
        }
      ],
      "source": [
        "scores = model.predict(xvalid_pad)\n",
        "scores_model.append({'Model': 'BERT','AUC_Score' : roc_auc(scores,yvalid)})\n",
        "scores_model"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
